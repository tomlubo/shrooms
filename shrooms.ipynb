{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94fc7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  # add import\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from ShroomDataset import ShroomDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report, top_k_accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "data_path = './data/'\n",
    "\n",
    "train_meta = pd.read_csv(os.path.join(data_path, 'train.csv'))#.iloc[:1000]\n",
    "val_meta = pd.read_csv(os.path.join(data_path, 'val.csv'))\n",
    "test_meta = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
    "\n",
    "# val_meta = train_meta\n",
    "# test_meta = train_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ffdcc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ShroomDataset(Dataset):\n",
    "\n",
    "#     def __init__(self, df, base_path = './data/', transform = None, label2idx = None):\n",
    "#         self.df = df\n",
    "#         self.base_path = base_path\n",
    "#         self.transform = transform\n",
    "\n",
    "#         if label2idx is None:\n",
    "#             unique = sorted(self.df['label'].unique())\n",
    "#             self.label2idx = {label : idx for idx, label in enumerate(unique)}\n",
    "#         else:\n",
    "#             self.label2idx = label2idx\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "        \n",
    "#         row  =  self.df.iloc[index]\n",
    "\n",
    "#         img_path = self.base_path + row['image_path']\n",
    "#         img = Image.open(img_path)\n",
    "#         img = ImageOps.exif_transpose(img)       # handle camera rotation\n",
    "#         if img.mode != 'RGB':                    # <- key line\n",
    "#             img = img.convert('RGB')\n",
    "\n",
    "#         if self.transform:\n",
    "#             img = self.transform(img)\n",
    "        \n",
    "#         label = self.label2idx[row['label']]\n",
    "\n",
    "#         return img, label\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a901dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes: 169\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_SIZE = 244  # try 320/384/448\n",
    "mean = (0.485, 0.456, 0.406)  # replace with dataset stats later\n",
    "std  = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_tf = T.Compose([\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.7,1.0), ratio=(0.75,1.33), antialias=True),\n",
    "    T.RandomHorizontalFlip(0.5),\n",
    "    T.RandomApply([T.ColorJitter(0.2,0.2,0.2,0.05)], p=0.4),\n",
    "    T.RandomApply([T.RandomRotation(15)], p=0.3),\n",
    "    T.RandomApply([T.RandomPerspective(0.25)], p=0.15),\n",
    "    T.RandomApply([T.GaussianBlur(3)], p=0.15),\n",
    "    T.ToDtype(torch.float32, scale=True),   # scales [0..255] -> [0..1]\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "val_tf = T.Compose([\n",
    "    T.Resize(IMG_SIZE, antialias=True),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToDtype(torch.float32, scale=True),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = ShroomDataset(train_meta, data_path, transform=train_tf)\n",
    "val_ds   = ShroomDataset(val_meta,   data_path, transform=val_tf, label2idx=train_ds.label2idx)\n",
    "test_ds  = ShroomDataset(test_meta,  data_path, transform=val_tf, label2idx=train_ds.label2idx)\n",
    "\n",
    "num_classes = len(train_ds.label2idx)\n",
    "idx2label = {v:k for k,v in train_ds.label2idx.items()}\n",
    "class_names = [idx2label[i] for i in range(num_classes)]\n",
    "\n",
    "print(f\"# classes: {num_classes}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle = True, num_workers=6, persistent_workers=True)\n",
    "val_loader = DataLoader(val_ds, batch_size = 32, num_workers=6, persistent_workers=True)\n",
    "test_loader = DataLoader(test_ds, batch_size = 32, num_workers=6, persistent_workers=True)\n",
    "# BATCH_SIZE = 32  # try 24/32/48 depending on memory\n",
    "# NUM_WORKERS = 4\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "#                           num_workers=NUM_WORKERS, persistent_workers=True, pin_memory=False)\n",
    "# val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "#                           num_workers=NUM_WORKERS, persistent_workers=True, pin_memory=False)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "#                           num_workers=NUM_WORKERS, persistent_workers=True, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7efdeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block using 1×1 convolutions for efficiency.\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Squeeze: global spatial info\n",
    "        y = self.avg_pool(x)\n",
    "        # Excitation & scale\n",
    "        y = self.fc(y)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module (CBAM) with streamlined operations.\"\"\"\n",
    "    def __init__(self, channels, reduction=16, kernel_size=5):\n",
    "        super().__init__()\n",
    "        # Channel attention\n",
    "        self.channel_attn = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.BatchNorm2d(channels // reduction),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # Spatial attention\n",
    "        self.spatial_attn = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel attention: refine channel weights\n",
    "        ca = self.channel_attn(x)\n",
    "        x = x * ca\n",
    "\n",
    "        # Spatial attention: refine spatial focus\n",
    "        max_pool, _ = x.max(dim=1, keepdim=True)\n",
    "        avg_pool = x.mean(dim=1, keepdim=True)\n",
    "        sa = self.spatial_attn(torch.cat([max_pool, avg_pool], dim=1))\n",
    "        return x * sa\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_se=False, reduction=16):\n",
    "        super().__init__()\n",
    "        self.use_se = use_se\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "        if use_se:\n",
    "            self.se = SEBlock(channels, reduction)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.use_se:\n",
    "            out = self.se(out)\n",
    "        return self.act(x + out)\n",
    "\n",
    "\n",
    "\n",
    "class ConvAttnBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Modular conv block:\n",
    "      1) Conv → BN → GELU\n",
    "      2) Optional CBAM attention\n",
    "      3) Optional ResidualBlock (with SE)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 use_cbam=False, use_res=False, use_se=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.attn = CBAM(out_channels) if use_cbam else nn.Identity()\n",
    "        self.res  = ResidualBlock(out_channels, use_se=use_se) if use_res else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.res(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ShroomCNNAttentive(nn.Module):\n",
    "    def __init__(self, in_ch=3, block_cfgs=None,\n",
    "                 mlp_units=(512,), num_classes=162):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 32, kernel_size=3, stride=2, padding=1, bias=False),  # down to 192×192\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        layers, ch = [], in_ch\n",
    "        ch = 64\n",
    "        for out_ch, do_pool, cbam, res, se in block_cfgs:\n",
    "            layers.append(ConvAttnBlock(ch, out_ch,\n",
    "                                       use_cbam=cbam,\n",
    "                                       use_res=res,\n",
    "                                       use_se=se))\n",
    "            if do_pool:\n",
    "                # stride-2 conv instead of MaxPool for more capacity\n",
    "                layers.append(nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=2, padding=1, bias=False))\n",
    "                layers.append(nn.BatchNorm2d(out_ch))\n",
    "                layers.append(nn.GELU())\n",
    "            ch = out_ch\n",
    "        self.model       = nn.Sequential(*layers)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # classifier\n",
    "        mlp = []\n",
    "        in_feat = ch\n",
    "        for u in mlp_units:\n",
    "            mlp += [nn.Linear(in_feat, u),\n",
    "                    nn.BatchNorm1d(u),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(0.4)]\n",
    "            in_feat = u\n",
    "        mlp.append(nn.Linear(in_feat, num_classes))\n",
    "        self.classifier = nn.Sequential(*mlp)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.name = 'Attentive CNN'\n",
    "    \n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            if getattr(m, 'bias', None) is not None: nn.init.zeros_(m.bias)\n",
    "        if isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "            nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.model(x)\n",
    "        x = self.global_pool(x).view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "\n",
    "blocks = [\n",
    "    # (out_ch, downsample, CBAM, Residual, SE)\n",
    "    (64,   True,  False, True,  True),   # light attention, residuals on\n",
    "    (128,  True,  False, True,  True),\n",
    "    (256,  True,  True,  True,  True),   # start CBAM here\n",
    "    (384,  True,  True,  True,  True),\n",
    "]\n",
    "# add more models to compare here\n",
    "models = { 'attentive': ShroomCNNAttentive(in_ch=3, block_cfgs=blocks,\n",
    "                    mlp_units=[1024, 512], num_classes=num_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ff40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Sharpness-Aware Minimization (SAM) optimizer wrapper.\n",
    "    Wraps any base optimizer (e.g. SGD or AdamW) to perform the two-step SAM update.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        \"\"\"\n",
    "        params         : iterable of parameters to optimize\n",
    "        base_optimizer : torch.optim.Optimizer class (not instance), e.g. torch.optim.SGD\n",
    "        rho            : SAM neighborhood size\n",
    "        kwargs         : arguments for the base optimizer (lr, momentum, weight_decay, etc.)\n",
    "        \"\"\"\n",
    "        assert rho >= 0.0, \"rho must be non-negative\"\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super().__init__(params, defaults)\n",
    "        # instantiate your base optimizer with the same param groups\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=True):\n",
    "        \"\"\"\n",
    "        1) Ascent step: move to the worst‐case neighbor w + ε\n",
    "        \"\"\"\n",
    "        # 1a) Compute the L2 norm of all gradients\n",
    "        grad_norm = torch.norm(\n",
    "            torch.stack([\n",
    "                p.grad.norm(p=2)\n",
    "                for group in self.param_groups for p in group['params']\n",
    "                if p.grad is not None\n",
    "            ]),\n",
    "            p=2\n",
    "        )\n",
    "        scale = self.param_groups[0]['rho'] / (grad_norm + 1e-12)\n",
    "\n",
    "        # 1b) Perturb each parameter by ε = scale * grad\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)                  # w = w + ε\n",
    "                self.state[p]['eps'] = e_w   # store ε for the second step\n",
    "\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=True):\n",
    "        \"\"\"\n",
    "        2) Descent step: take an optimizer step from the perturbed weights,\n",
    "           then restore w ← w + ε − ε = the new w.\n",
    "        \"\"\"\n",
    "        # 2a) descent on the perturbed weights\n",
    "        self.base_optimizer.step()\n",
    "\n",
    "        # 2b) subtract ε to return to the updated original weights\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                eps = self.state[p].get('eps')\n",
    "                if eps is None:\n",
    "                    continue\n",
    "                p.sub_(eps)\n",
    "\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        We don’t use this—call first_step() and second_step() explicitly.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Use first_step() and second_step() instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aef6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          train_loader,\n",
    "          val_loader,\n",
    "          epochs: int = 50,\n",
    "          lr: float = 1e-3,\n",
    "          weight_decay: float = 5e-4):\n",
    "    \"\"\"\n",
    "    Train `model` with SAM, logging per-epoch loss & accuracy.\n",
    "    Returns: (train_loss_hist, train_acc_hist, val_loss_hist, val_acc_hist)\n",
    "    \"\"\"\n",
    "    # 1) Device setup\n",
    "    device = 'mps' if torch.backends.mps.is_available() else \\\n",
    "             'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"[Training] using device = {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 2) Loss, optimizer, scheduler\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    base_opt = optim.AdamW\n",
    "    optimizer = SAM(\n",
    "        model.parameters(),\n",
    "        base_optimizer=base_opt,\n",
    "        rho=0.06,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer.base_optimizer,   # <- important with SAM\n",
    "        mode='min',\n",
    "        factor=0.5,                 # LR *= 0.5  (try 0.2–0.5)\n",
    "        patience=3,                 # epochs with no val improvement before reducing\n",
    "        threshold=1e-3,             # “min improvement” to count as progress\n",
    "        cooldown=0,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    # 3) History buffers\n",
    "    train_loss_hist, train_acc_hist = [], []\n",
    "    val_loss_hist,   val_acc_hist   = [], []\n",
    "\n",
    "    # 4) Epoch loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss, running_correct, running_total = 0.0, 0, 0\n",
    "        i = 0\n",
    "        for X, y in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # ---- step 1: forward & backward on current weights ----\n",
    "            logits = model(X)\n",
    "            loss   = loss_fn(logits, y)\n",
    "            loss.backward()\n",
    "            # record train-batch accuracy\n",
    "            preds = logits.argmax(dim=1)\n",
    "            running_correct += (preds == y).sum().item()\n",
    "            running_total   += y.size(0)\n",
    "\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "\n",
    "            # ---- step 2: forward & backward on perturbed weights ----\n",
    "            logits2 = model(X)\n",
    "            loss2   = loss_fn(logits2, y)\n",
    "            loss2.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "\n",
    "            running_loss += loss2.item() * y.size(0)\n",
    "            # print(f\"Epoch {epoch}: loss={running_loss/running_total:.4f}, \"\n",
    "            #     f\"acc={running_correct/running_total:.4f}\")\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / running_total\n",
    "        epoch_acc  = running_correct / running_total\n",
    "        train_loss_hist.append(epoch_loss)\n",
    "        train_acc_hist.append(epoch_acc)\n",
    "\n",
    "        # ---- validation ----\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for Xv, yv in tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False):\n",
    "                Xv, yv = Xv.to(device), yv.to(device)\n",
    "                out = model(Xv)\n",
    "                val_loss += loss_fn(out, yv).item() * yv.size(0)\n",
    "                val_correct += (out.argmax(dim=1) == yv).sum().item()\n",
    "                val_total   += yv.size(0)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        scheduler.step(val_loss)\n",
    "        val_acc   = val_correct / val_total\n",
    "        val_loss_hist.append(val_loss)\n",
    "        val_acc_hist.append(val_acc)\n",
    "        curr_lr = optimizer.base_optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(f\"Epoch {epoch:3d}/{epochs:3d} | \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n",
    "              f\"LR: {curr_lr:.6g}\")\n",
    "        \n",
    " \n",
    "\n",
    "    return train_loss_hist, train_acc_hist, val_loss_hist, val_acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5fb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(models, train_loader, val_loader, epochs = 20, lr = 0.001):\n",
    "\n",
    "    ''' trains a dictionary of models with the given params'''\n",
    "    \n",
    "    model_history = {}\n",
    "    for model in models.values():\n",
    "        start_time = time.time()\n",
    "        print('######################################################')\n",
    "        print(f'###         TRAINING MODEL: {model.name}          ###')\n",
    "        print('######################################################')\n",
    "\n",
    "        history = train(model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                epochs=epochs,\n",
    "                lr=lr)\n",
    "        \n",
    "        model_history[model.name] = history\n",
    "\n",
    "        print('######################################################')\n",
    "        print(f'##         TIME TO TRAIN MODEL:{model.name}       ###')\n",
    "        print(f'###     {(time.time() - start_time)/60} MINUTES   ###')\n",
    "        print('######################################################')\n",
    "\n",
    "    return model_history\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bff9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, test_loader, classes = idx2label.values()):\n",
    "\n",
    "    ''' evaluate the models on never seen test data'''\n",
    "\n",
    "    for model in models.values():\n",
    "        print('######################################################')\n",
    "        print(f'###         EVALUATING MODEL: {model.name}          ###')\n",
    "        print('######################################################')\n",
    "        \n",
    "        # set device\n",
    "        device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # collect predictions and true labels to calculate metrics\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_logits = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # load data\n",
    "            for X, y in test_loader:\n",
    "                # to gpu\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # make predictions\n",
    "                logits = model(X)\n",
    "\n",
    "                # get most confident\n",
    "                preds = logits.argmax(dim=1)\n",
    "\n",
    "                y_logits.append(logits.cpu())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "\n",
    "        # convert stacked logits\n",
    "        y_logits = torch.cat(y_logits).numpy()\n",
    "\n",
    "        # top-k accuracy\n",
    "        top1_acc = accuracy_score(y_true, y_pred)\n",
    "        top5_acc = top_k_accuracy_score(y_true, y_logits, k=5, labels=list(range(len(classes))))  # labels=class range\n",
    "\n",
    "        # other classification metrics from sklearn\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        precision = precision_score(y_true, y_pred, average='weighted')\n",
    "        recall = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "        print(f\"\\nTest Accuracy (Top-1): {top1_acc:.4f}\")\n",
    "        print(f\"Top-5 Accuracy: {top5_acc:.4f}\")\n",
    "        print(f\"F1 Score (weighted): {f1:.4f}\")\n",
    "        print(f\"Precision (weighted): {precision:.4f}\")\n",
    "        print(f\"Recall (weighted): {recall:.4f}\")\n",
    "\n",
    "        # per-class performance\n",
    "        print(\"\\nPer-Class Report:\")\n",
    "        cl_report = classification_report(y_true, y_pred, target_names=classes)\n",
    "        print(cl_report)\n",
    "    return (top1_acc, top5_acc, f1, precision, recall, cl_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fca5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models, train_loader, val_loader, test_loader, epochs=50, lr=0.001, class_names=None):\n",
    "\n",
    "    # train all models\n",
    "    model_history = train_models(models, train_loader, val_loader, epochs=epochs, lr=lr)\n",
    "\n",
    "    # evaluate all models\n",
    "    eval_results = {}\n",
    "    for name, model in models.items():\n",
    "        print(\"\\n\\n\")\n",
    "        results = evaluate_models({name: model}, test_loader, classes=class_names)\n",
    "        eval_results[name] = results\n",
    "\n",
    "    # plot training curves\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    for model_name, history in model_history.items():\n",
    "        train_loss, train_acc, val_loss, val_acc = history\n",
    "        axs[0].plot(train_loss, label=f'{model_name} - Train' )\n",
    "        axs[0].plot(val_loss, label=f'{model_name} - Val',marker='o', markersize=4 )\n",
    "        axs[1].plot(train_acc, label=f'{model_name} - Train')\n",
    "        axs[1].plot(val_acc, label=f'{model_name} - Val', marker='o', markersize=4)\n",
    "\n",
    "    axs[0].set_title('Loss Curves')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].set_title('Accuracy Curves')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # return eval results and training history for data logging\n",
    "    return model_history, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bef758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################\n",
      "###         TRAINING MODEL: Attentive CNN          ###\n",
      "######################################################\n",
      "[Training] using device = mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a775f7568359411989985280fc073934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/25:   0%|          | 0/21548 [00:23<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a995081d1c2416796d5a9985ffe4ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/25:   0%|          | 0/488 [00:08<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/ 25 | Train Loss: 4.4582, Train Acc: 0.0705 | Val Loss: 3.5042, Val Acc: 0.1816 | LR: 0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8c8f18ce4d4761bd5b5cc529b86ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/25:   0%|          | 0/21548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661670a0594d4d09993dcd118b308326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/25:   0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/ 25 | Train Loss: 3.6754, Train Acc: 0.1767 | Val Loss: 2.9971, Val Acc: 0.2734 | LR: 0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c658610001604d32a7d79efd4bc9dca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/25:   0%|          | 0/21548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "lr = 0.01\n",
    "\n",
    "model_history, eval_results = compare_models(models, train_loader, val_loader, test_loader,\n",
    "                                             epochs=num_epochs, lr=lr, class_names=idx2label.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db36a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc218597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
